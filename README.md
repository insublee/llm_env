# ğŸ§  LLM Fine-tuning & vLLM Serving (Docker ê¸°ë°˜)

ì´ ë ˆí¬ëŠ” LoRA ê¸°ë°˜ LLM íŒŒì¸íŠœë‹ â†’ ë³‘í•© â†’ vLLM ì„œë¹™ê¹Œì§€ë¥¼
Docker + GPU í™˜ê²½ì—ì„œ í•œ ë²ˆì— ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

âœ… GPU 1ì¥ (ì˜ˆ: RTX 4090)
âœ… Docker Desktop + WSL2
âœ… NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜ ì™„ë£Œ
ì´ 3ê°€ì§€ë§Œ ë˜ì–´ ìˆìœ¼ë©´ ë©ë‹ˆë‹¤.

# ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°
```
llm_env/
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ Dockerfile.base
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â””â”€â”€ merge_lora.py
â”‚   â””â”€â”€ vllm/
â”‚       â””â”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.train.yml
â”œâ”€â”€ docker-compose.vllm.yml
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ lora/
â”‚   â”‚   â””â”€â”€ my_model/        # LoRA í•™ìŠµ ê²°ê³¼
â”‚   â””â”€â”€ merged/
â”‚       â””â”€â”€ my_model/        # LoRA ë³‘í•© ì™„ë£Œ ëª¨ë¸ (vLLMìš©)
â””â”€â”€ README.md
```
# 1ï¸âƒ£ ì‚¬ì „ ì¤€ë¹„ (í•œ ë²ˆë§Œ)
## 1. Docker & GPU í™•ì¸
```
docker --version
nvidia-smi
```

Docker Desktop ì„¤ì •:
Settings â†’ Resources â†’ Advanced
Docker data locationì„ **ì—¬ìœ  ìˆëŠ” ë””ìŠ¤í¬ (ì˜ˆ: E:)**ë¡œ ì„¤ì • ê¶Œì¥

# 2ï¸âƒ£ í•™ìŠµ (LoRA Fine-tuning)
## 2-1. ì´ì „ ì»¨í…Œì´ë„ˆ / ë³¼ë¥¨ ì •ë¦¬ (ì¤‘ìš”)
```
docker compose -f docker-compose.train.yml down -v
rm -rf models/*
```
## 2-2. í•™ìŠµ ì‹¤í–‰
```
docker compose -f docker-compose.train.yml up --build
```

ì •ìƒì ìœ¼ë¡œ ëŒë©´ ë§ˆì§€ë§‰ì— ë‹¤ìŒ ë¡œê·¸ê°€ ë³´ì…ë‹ˆë‹¤:
```
ğŸ‰ Training done!
llm_train exited with code 0
```
## 2-3. LoRA ê²°ê³¼ í™•ì¸
```
ls models/lora/my_model
```

ì•„ë˜ íŒŒì¼ë“¤ì´ ìˆìœ¼ë©´ ì •ìƒì…ë‹ˆë‹¤:

adapter_config.json
adapter_model.safetensors
tokenizer.json
tokenizer.model

# 3ï¸âƒ£ LoRA â†’ Base ëª¨ë¸ ë³‘í•© (í•„ìˆ˜)

vLLMì€ LoRA ìƒíƒœì˜ ëª¨ë¸ì„ ì§ì ‘ ì„œë¹™í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
ë°˜ë“œì‹œ mergeê°€ í•„ìš”í•©ë‹ˆë‹¤.

## 3-1. ë³‘í•© ì‹¤í–‰
```
docker compose -f docker-compose.train.yml run --rm llm_train python merge_lora.py
```

ì •ìƒ ë¡œê·¸:
```
ğŸ”— Loading LoRA adapter...
ğŸ§¬ Merging LoRA into base model...
ğŸ’¾ Saving merged model...
ğŸ‰ Merge complete!
```
## 3-2. ë³‘í•© ê²°ê³¼ í™•ì¸
```
ls models/merged/my_model
```

ì•„ë˜ íŒŒì¼ë“¤ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤:
```
config.json
model.safetensors (ë˜ëŠ” shard íŒŒì¼ë“¤)
tokenizer.json
tokenizer.model
generation_config.json
```

â— adapter_* íŒŒì¼ì´ ì—†ì–´ì•¼ ì •ìƒì…ë‹ˆë‹¤.

# 4ï¸âƒ£ vLLM ì„œë¹™ ì‹¤í–‰
## 4-1. vLLM ì»¨í…Œì´ë„ˆ ì‹¤í–‰
```
docker compose -f docker-compose.vllm.yml up --build
```

ì •ìƒ ë¡œê·¸ ì˜ˆì‹œ:
```
vLLM API server version 0.12.0
Listening on http://0.0.0.0:8000
```

## 4-2. API í…ŒìŠ¤íŠ¸
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "my_model",
    "prompt": "Explain LoRA fine-tuning in simple terms.",
    "max_tokens": 200
  }'
---

# âš ï¸ ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ
## âŒ ëª¨ë¸ì´ ì €ì¥ë˜ì§€ ì•ŠëŠ” ê²½ìš°

TrainingArguments.output_dir ì™€

trainer.save_model() ê²½ë¡œê°€ ì»¨í…Œì´ë„ˆ ê¸°ì¤€ ê²½ë¡œì¸ì§€ í™•ì¸
```
output_dir="/models/lora/my_model"
trainer.save_model("/models/lora/my_model")
```
## âŒ vLLMì—ì„œ /models/my_model ì—ëŸ¬

config.json ì—†ëŠ” ë””ë ‰í† ë¦¬ë¥¼ ê°€ë¦¬í‚¤ê³  ìˆëŠ” ê²½ìš°

ë°˜ë“œì‹œ merged ëª¨ë¸ ê²½ë¡œ ì‚¬ìš©

command: vllm serve /models/merged/my_model
---
# ğŸ¯ ì „ì²´ íŒŒì´í”„ë¼ì¸ ìš”ì•½
```
train.py
  â†“
LoRA adapters ìƒì„±
  â†“
merge_lora.py
  â†“
ìˆœìˆ˜ HF ëª¨ë¸ ìƒì„±
  â†“
vLLM ì„œë¹™
```
