# LLM Training & Serving Environment (WSL + Docker + GPU)

Windows + WSL2 í™˜ê²½ì—ì„œ
**LLM í•™ìŠµ(Unsloth)ê³¼ ì„œë¹™(vLLM)ì„ Dockerë¡œ ë¶„ë¦¬** ìš´ì˜í•˜ê¸° ìœ„í•œ ê°œë°œ í™˜ê²½ì…ë‹ˆë‹¤.

RTX 4090 ê¸°ì¤€ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ë˜ì—ˆìœ¼ë©°,
ì¬í˜„ ê°€ëŠ¥í•œ í™˜ê²½ êµ¬ì„±ì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.

## ğŸ§± Architecture Overview
``` scss
Windows
 â””â”€ WSL2 (Ubuntu)
     â””â”€ Docker Desktop (WSL backend, data on E:)
         â”œâ”€ llm-train  (Unsloth + GPU fine-tuning)
         â”œâ”€ llm-vllm   (vLLM inference server)
         â””â”€ llm-api    (FastAPI, optional)
```

**Training / Serving ì™„ì „ ë¶„ë¦¬**

Docker image rebuild ìµœì†Œí™”

ëŒ€ìš©ëŸ‰ ìºì‹œ(HuggingFace, pip) â†’ ì™¸ë¶€ ë³¼ë¥¨ ë§ˆìš´íŠ¸

## ğŸ’» Requirements
### Hardware
- NVIDIA GPU (tested: RTX 4090, 24GB VRAM)
- SSD ê¶Œì¥ (Docker + HF cache)

### Software
- Windows 11
- WSL2 (Ubuntu 22.04)
- Docker Desktop (WSL backend)
- NVIDIA GPU Driver (Windows)
- NVIDIA Container Toolkit (Docker Desktop í¬í•¨)

## ğŸ“ Project Structure
``` csharp
llm_env/
â”œâ”€ docker/
â”‚  â””â”€ train/
â”‚     â”œâ”€ Dockerfile.base   # heavy deps (torch, unsloth)
â”‚     â”œâ”€ Dockerfile        # lightweight training image
â”‚     â””â”€ requirements.txt
â”œâ”€ docker-compose.train.yml
â”œâ”€ train.py
â”œâ”€ models/                 # trained models output
â””â”€ README.md
```

## ğŸš€ Training (Unsloth)
### 1ï¸âƒ£ Build base image (1íšŒë§Œ)
``` bash
cd docker/train

DOCKER_BUILDKIT=1 docker build \
  -f Dockerfile.base \
  -t llm-train-base .
```

âš ï¸ ì´ ë‹¨ê³„ëŠ” ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ (torch, unsloth, triton)

---

### 2ï¸âƒ£ Run training container
``` bash
cd /mnt/e/llm/llm_env

docker compose -f docker-compose.train.yml up --build
```
- GPU ìë™ ì¸ì‹
- HuggingFace cache ì™¸ë¶€ ë§ˆìš´íŠ¸
- ëª¨ë¸ ì¶œë ¥: ```./models/```

---

### ğŸ§  Training Details

Model: unsloth/mistral-7b-v0.3
Dataset: HuggingFaceH4/ultrachat_200k (train_sft[:1000])
Precision: bf16
Fine-tuning: LoRA (PEFT)
Gradient checkpointing: unsloth

### í•µì‹¬ ì„¤ì • (train.py)
``` python
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=ds,
    max_seq_length=2048,
    args=TrainingArguments(
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        max_steps=100,
        learning_rate=2e-4,
        bf16=True,
        gradient_checkpointing="unsloth",
        output_dir="/models/my_model",
        save_steps=50,
        save_total_limit=2,
        logging_steps=5,
        report_to="none",
    ),
)
```

## ğŸ“¦ Volume & Cache Strategy
### docker-compose.train.yml
``` yaml
volumes:
  - ./models:/models
  - /mnt/e/hf_cache:/root/.cache/huggingface

environment:
  - HF_HOME=/root/.cache/huggingface
```
- Docker rebuild ì‹œì—ë„ HF ëª¨ë¸ ì¬ë‹¤ìš´ë¡œë“œ ë°©ì§€
- SSD(E:) ì‚¬ìš© ê¶Œì¥

## ğŸ§¯ Known Pitfalls & Fixes
âŒ ```cannot find -lcuda```

âœ” í•´ê²°:
nvidia/cuda:*runtime* âŒ
nvidia/cuda:*devel* âœ…
libcuda.so WSL symlink í•„ìš”
``` dockerfile
FROM nvidia/cuda:12.1.0-devel-ubuntu22.04
RUN ln -s /usr/lib/wsl/lib/libcuda.so /usr/lib/libcuda.so || true
```
---
âŒ **Unsloth dependency conflict**

âœ” í•´ê²°:
- torch, trl ë²„ì „ ì§ì ‘ ê³ ì •í•˜ì§€ ë§ ê²ƒ
- unslothê°€ ìš”êµ¬í•˜ëŠ” ë²„ì „ì— ë§¡ê¸°ê¸°
---
âŒ **Quantized model cannot be fine-tuned**

âœ” í•´ê²°:
- LoRA adapters ë°˜ë“œì‹œ í™œì„±í™”
- pure 4bit ëª¨ë¸ ë‹¨ë… í•™ìŠµ âŒ
---

## âœ… Current Status

- DONE WSL + Docker + GPU ì •ìƒ ì¸ì‹
- DONE  Unsloth fine-tuning ì„±ê³µ
- DONE ëª¨ë¸ ì €ì¥ í™•ì¸ (/models)
- TODO vLLM ì„œë¹™ ì—°ê²°
- TODO HuggingFace ìë™ ì—…ë¡œë“œ
- TODO FastAPI ì¸ì¦ / ë¡œê·¸

## ğŸ”œ Next Steps

1. vLLM ì»¨í…Œì´ë„ˆì—ì„œ /models ë¡œì»¬ ëª¨ë¸ ë¡œë”©
2. OpenAI-compatible API í…ŒìŠ¤íŠ¸
3. HuggingFace Hub ìë™ push
4. ì‹¤ì‚¬ìš©ìš© config ë¶„ë¦¬ (dev / prod)
---

**ğŸ§  Notes**

ì´ ë ˆí¬ëŠ” **â€œí•œ ë²ˆ ì„¸íŒ…í•˜ë©´ ë‹¤ì‹œ ì•ˆ ê¹¨ì§€ëŠ” LLM ì‹¤í—˜ í™˜ê²½â€**ì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.
Windows + GPU + Docker + LLM ì¡°í•©ì—ì„œ ì‚½ì§ˆì„ ì¤„ì´ê¸° ìœ„í•œ ê¸°ë¡ì…ë‹ˆë‹¤.
