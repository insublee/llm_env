services:
  train:
    build: ./docker/train
    container_name: llm_train
    volumes:
      - ${DATASETS_DIR}:/workspace/datasets
      - ${MODELS_DIR}:/models
      - ${LOGS_DIR}:/workspace/logs
      - ./configs:/workspace/configs
      - ./outputs:/workspace/outputs
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HF_HOME=/workspace/.hf
      - TRANSFORMERS_CACHE=/workspace/.hf
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    command: ["python", "train.py"]

  vllm:
    build: ./docker/vllm
    container_name: llm_vllm
    ports:
      - "${VLLM_PORT}:8000"
    volumes:
      - ${MODELS_DIR}:/models
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    command: [
      "vllm",
      "serve",
      "/models/${MODEL_NAME}",
      "--host",
      "${VLLM_HOST}",
      "--port",
      "8000"
    ]

  api:
    build: ./serve
    container_name: llm_api
    ports:
      - "${API_PORT}:9000"
    volumes:
      - ${MODELS_DIR}:/models
    environment:
      - VLLM_URL=http://vllm:8000/generate
    depends_on:
      - vllm
    command: ["python", "app.py"]

  webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: llm_webui
    ports:
      - "${WEBUI_PORT}:8080"
    environment:
      - OPENAI_API_BASE_URL=http://vllm:8000/v1
      - OPENAI_API_KEY=sk-local
    depends_on:
      - vllm
    restart: unless-stopped

volumes:
  datasets:
  models:
  logs:

